#!/usr/bin/env python
# -*- coding: utf-8 -*-
import asyncio
import json
import sys
import logging
import aiohttp
from datetime import datetime, timedelta
import os
from pathlib import Path
import pandas as pd
import time
from typing import Optional, Dict, List, Any
import re
import twitter_api
import hmac
import hashlib
import base64
import requests
from feishu_bot import FeishuBot
import lark_oapi as lark
from lark_oapi.api.im.v1 import CreateMessageRequest, CreateMessageRequestBody

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

__all__ = ['MemeAnalyzer', 'BacktestProcessor', 'process_message']

class MemeAnalyzer:
    def __init__(self, config_file='config.json', api_key=None):
        self.config = self.load_config(config_file)
        self.setup_directories()
        
        # APIé…ç½®
        self.base_url = self.config.get("base_url", "https://api.siliconflow.cn")
        self.api_key = api_key or self.config.get("api_keys", {}).get("deepseek")
        
        # éªŒè¯ API key æ ¼å¼
        if not self.api_key:
            logger.error("Deepseek API keyæœªè®¾ç½®")
            raise ValueError("Deepseek API key is required")
        elif not self.api_key.startswith("sk-"):
            logger.error("Deepseek API key æ ¼å¼é”™è¯¯ï¼Œåº”è¯¥ä»¥ sk- å¼€å¤´")
            raise ValueError("Invalid Deepseek API key format")
            
        logger.info(f"Deepseek API key æ ¼å¼éªŒè¯é€šè¿‡")
        
        self.min_occurrence_threshold = self.config.get("min_occurrence_threshold", 2)
        self.term_history = {}
        self.history_cleanup_threshold = timedelta(hours=self.config.get("history_cleanup_threshold", 24))
        
        # æ·»åŠ é£ä¹¦é…ç½®
        self.feishu_webhook = self.config.get("feishu_webhook", "")
        self.feishu_secret = self.config.get("feishu_secret", "")

    def load_config(self, config_file):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8-sig') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise

    def setup_directories(self):
        """è®¾ç½®å¿…è¦çš„ç›®å½•"""
        self.data_dir = Path('data')
        self.data_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®Excelæ–‡ä»¶è·¯å¾„
        self.twitter_results_path = self.data_dir / 'twitter_results.xlsx'
        self.meme_path = self.data_dir / 'meme.xlsx'
        self.analysis_path = self.data_dir / 'crypto_analysis_results.xlsx'

    def send_to_feishu(self, analysis_result):
        """å‘é€åˆ†æç»“æœåˆ°é£ä¹¦"""
        try:
            timestamp = str(int(time.time()))
            
            # è®¡ç®—ç­¾å
            string_to_sign = f"{timestamp}\n{self.feishu_secret}"
            hmac_code = hmac.new(
                string_to_sign.encode("utf-8"),
                digestmod=hashlib.sha256
            ).digest()
            sign = base64.b64encode(hmac_code).decode('utf-8')
            
            # æ ¼å¼åŒ–æ¶ˆæ¯å†…å®¹
            message = f"""ğŸ” Meme å¸åˆ†ææŠ¥å‘Š
å…³é”®è¯: {analysis_result['æœç´¢å…³é”®è¯']}

ğŸ“ å™äº‹ä¿¡æ¯:
{analysis_result['å™äº‹ä¿¡æ¯']}

ğŸŒ¡ï¸ å¯æŒç»­æ€§åˆ†æ:
â€¢ ç¤¾åŒºçƒ­åº¦: {analysis_result['å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦']}
â€¢ ä¼ æ’­æ½œåŠ›: {analysis_result['å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›']}
â€¢ çŸ­æœŸæŠ•æœºä»·å€¼: {analysis_result['å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼']}

ğŸ“Š æ•°æ®ç»Ÿè®¡:
â€¢ åŸå§‹æ¨æ–‡æ•°é‡: {analysis_result['åŸå§‹æ¨æ–‡æ•°é‡']}
â€¢ åˆ†ææ—¶é—´: {analysis_result['åˆ†ææ—¶é—´']}"""

            headers = {
                "Content-Type": "application/json"
            }
            
            payload = {
                "timestamp": timestamp,
                "sign": sign,
                "msg_type": "text",
                "content": {
                    "text": message
                }
            }
            
            response = requests.post(
                self.feishu_webhook,
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                logger.info("æˆåŠŸå‘é€åˆ†æç»“æœåˆ°é£ä¹¦")
            else:
                logger.error(f"å‘é€åˆ°é£ä¹¦å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            logger.error(f"å‘é€åˆ°é£ä¹¦æ—¶å‡ºé”™: {str(e)}")

    async def analyze_tweets(self, term: str, tweets: List[dict]) -> dict:
        """ä½¿ç”¨ Deepseek API åˆ†ææ¨æ–‡"""
        try:
            # æå–æ¨æ–‡å†…å®¹å¹¶æ¸…ç†
            tweet_texts = []
            for tweet in tweets:
                text = tweet.get('text', '').strip()
                if text:
                    # æ¸…ç†åˆçº¦åœ°å€
                    text = re.sub(r'[A-Za-z0-9]{32,}', '', text)
                    # æ¸…ç†URL
                    text = re.sub(r'https?://\S+', '', text)
                    # æ¸…ç†å¤šä½™ç©ºç™½
                    text = ' '.join(text.split())
                    if text.strip():  # ç¡®ä¿æ¸…ç†åè¿˜æœ‰å†…å®¹
                        tweet_texts.append(text)
            
            if not tweet_texts:
                logger.warning(f"æ¸…ç†åæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¨æ–‡å†…å®¹ç”¨äºåˆ†æ")
                return self._get_default_analysis(term, len(tweets))
            
            # å»é‡
            tweet_texts = list(set(tweet_texts))
            
            # ä¿®æ”¹è®¤è¯å¤´æ ¼å¼
            headers = {
                "Authorization": f"Bearer {self.api_key}",  # ç¡®ä¿æ˜¯ Bearer è®¤è¯
                "Content-Type": "application/json",
                "Accept": "application/json"  # æ·»åŠ  Accept å¤´
            }
            
            # æ£€æŸ¥å¹¶è®°å½• API keyï¼ˆéšè—éƒ¨åˆ†å†…å®¹ï¼‰
            masked_key = f"{self.api_key[:6]}...{self.api_key[-4:]}" if self.api_key else "None"
            logger.info(f"ä½¿ç”¨çš„ API key: {masked_key}")
            
            data = {
                "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                "messages": [
                    {
                        "role": "user",
                        "content": f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸åˆ†æå¸ˆï¼Œæˆ‘å¸Œæœ›ä½ èƒ½å¸®æˆ‘è¯„ä¼°è¿™ä¸ª Meme å¸çš„æ½œåŠ›ï¼Œå¹¶ç»™å‡ºè¯¦ç»†çš„åˆ†æå’Œå»ºè®®ï¼Œè¯·åˆ†æä»¥ä¸‹å…³äºåŠ å¯†è´§å¸çš„æ¨æ–‡å†…å®¹ï¼š

{chr(10).join(tweet_texts)}

è¯·ä»ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢åˆ†åˆ«è¿›è¡Œåˆ†æï¼Œåˆ†2ç‚¹ï¼Œå¹¶ç”¨ä¸­æ–‡å›ç­”ï¼Œæˆ‘éœ€è¦çš„ç»“æœä¸è¶…è¿‡100å­—ï¼Œä½ éœ€è¦åˆ†ä»¥ä¸‹2ç‚¹æ˜ç¡®çš„è¿”å›ï¼š

1. å™äº‹ä¿¡æ¯ï¼šç”¨2-3å¥è¯æ€»ç»“è¿™ä¸ªmemeå¸çš„æ ¸å¿ƒå’Œå®ƒçš„æ ¸å¿ƒå–ç‚¹ã€‚

2. å¯æŒç»­æ€§ï¼šä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š
   - ç¤¾åŒºçƒ­åº¦
   - ä¼ æ’­æ½œåŠ›
   - çŸ­æœŸæŠ•æœºä»·å€¼"""
                    }
                ],
                "stream": False,
                "temperature": 0.7,
                "max_tokens": 512,
                "top_p": 0.7,
                "top_k": 50,
                "frequency_penalty": 0.5
            }
            
            logger.info(f"å‘é€Deepseek APIè¯·æ±‚ï¼Œåˆ†æ {len(tweet_texts)} æ¡æ¨æ–‡")
            
            max_retries = 3
            retry_delay = 10
            
            for attempt in range(max_retries):
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            f"{self.base_url}/v1/chat/completions",
                            headers=headers,
                            json=data,
                            timeout=aiohttp.ClientTimeout(total=30)
                        ) as response:
                            response_text = await response.text()
                            logger.info(f"Deepseek APIå“åº”çŠ¶æ€ç : {response.status}")
                            logger.info(f"Deepseek APIå“åº”å¤´: {dict(response.headers)}")
                            logger.info(f"Deepseek APIè¯·æ±‚æ•°æ®: {json.dumps(data, ensure_ascii=False)}")
                            logger.info(f"Deepseek APIå“åº”å†…å®¹: {response_text}")
                            
                            if response.status == 200:
                                result = json.loads(response_text)
                                if 'choices' in result and len(result['choices']) > 0:
                                    analysis = result['choices'][0]['message']['content']
                                    
                                    # è§£æåˆ†æç»“æœ
                                    narrative = ""
                                    community_heat = ""
                                    spread_potential = ""
                                    investment_value = ""
                                    
                                    # ç§»é™¤æ‰€æœ‰ Markdown æ ‡è®°
                                    analysis = analysis.replace('**', '')
                                    
                                    # åˆ†å‰²ä¸»è¦éƒ¨åˆ†
                                    parts = analysis.split('\n\n')
                                    
                                    # è§£æå™äº‹ä¿¡æ¯å’Œå¯æŒç»­æ€§è¯„ä¼°
                                    for part in parts:
                                        if '1. å™äº‹ä¿¡æ¯' in part:
                                            narrative = part.replace('1. å™äº‹ä¿¡æ¯ï¼š', '').strip()
                                        elif '2. å¯æŒç»­æ€§' in part:
                                            lines = part.split('\n')
                                            for line in lines:
                                                line = line.strip()
                                                if 'ç¤¾åŒºçƒ­åº¦' in line:
                                                    community_heat = line.split('ï¼š')[1].strip() if 'ï¼š' in line else ''
                                                elif 'ä¼ æ’­æ½œåŠ›' in line:
                                                    spread_potential = line.split('ï¼š')[1].strip() if 'ï¼š' in line else ''
                                                elif 'çŸ­æœŸæŠ•æœºä»·å€¼' in line:
                                                    investment_value = line.split('ï¼š')[1].strip() if 'ï¼š' in line else ''
                                    
                                    result_dict = {
                                        'æœç´¢å…³é”®è¯': term,
                                        'å™äº‹ä¿¡æ¯': narrative,
                                        'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦': community_heat,
                                        'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›': spread_potential,
                                        'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼': investment_value,
                                        'åŸå§‹æ¨æ–‡æ•°é‡': len(tweets),
                                        'åˆ†ææ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                    }
                                    
                                    logger.info(f"æˆåŠŸå®Œæˆåˆ†æ: {result_dict}")
                                    
                                    # åœ¨å®Œæˆåˆ†æåå‘é€åˆ°é£ä¹¦
                                    self.send_to_feishu(result_dict)
                                    
                                    return result_dict
                                    
                            elif response.status == 400:
                                logger.error(f"Deepseek APIè¯·æ±‚å‚æ•°é”™è¯¯: {response_text}")
                                try:
                                    error_data = json.loads(response_text)
                                    logger.error(f"é”™è¯¯è¯¦æƒ…: {json.dumps(error_data, ensure_ascii=False, indent=2)}")
                                except:
                                    logger.error(f"æ— æ³•è§£æé”™è¯¯å“åº”: {response_text}")
                                return self._get_default_analysis(term, len(tweets))
                            elif response.status == 401:
                                logger.error("Deepseek APIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥API key")
                                return self._get_default_analysis(term, len(tweets))
                            elif response.status == 429:
                                logger.warning("Deepseek APIé€Ÿç‡é™åˆ¶")
                                return self._get_default_analysis(term, len(tweets))
                            else:
                                logger.error(f"Deepseek APIè¯·æ±‚å¤±è´¥: {response.status}")
                                logger.error(f"é”™è¯¯å“åº”: {response_text}")
                                return self._get_default_analysis(term, len(tweets))
                                
                except Exception as e:
                    logger.error(f"è°ƒç”¨Deepseek APIæ—¶å‡ºé”™: {str(e)}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        continue
                    
            return self._get_default_analysis(term, len(tweets))
            
        except Exception as e:
            logger.error(f"åˆ†ææ¨æ–‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            logger.exception(e)
            return self._get_default_analysis(term, len(tweets))

    def _get_default_analysis(self, term: str, tweet_count: int) -> dict:
        """è¿”å›é»˜è®¤çš„åˆ†æç»“æœ"""
        return {
            'æœç´¢å…³é”®è¯': term,
            'å™äº‹ä¿¡æ¯': f'APIè®¤è¯å¤±è´¥ï¼Œæ— æ³•åˆ†æã€‚å…±æœ‰{tweet_count}æ¡æ¨æ–‡',
            'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦': 'æœªçŸ¥',
            'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›': 'æœªçŸ¥',
            'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼': 'æœªçŸ¥',
            'åŸå§‹æ¨æ–‡æ•°é‡': tweet_count,
            'åˆ†ææ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

    async def save_analysis_results(self, analysis_results: List[dict]):
        """ä¿å­˜åˆ†æç»“æœåˆ°Excel"""
        try:
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯»å–ç°æœ‰æ•°æ®å¹¶è¿½åŠ 
            if self.analysis_path.exists():
                existing_df = pd.read_excel(self.analysis_path)
                new_df = pd.DataFrame(analysis_results)
                df = pd.concat([existing_df, new_df], ignore_index=True)
            else:
                df = pd.DataFrame(analysis_results)
            
            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
            expected_columns = [
                'æœç´¢å…³é”®è¯', 
                'å™äº‹ä¿¡æ¯', 
                'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦', 
                'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›', 
                'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼',
                'åŸå§‹æ¨æ–‡æ•°é‡',
                'åˆ†ææ—¶é—´'
            ]
            
            for col in expected_columns:
                if col not in df.columns:
                    df[col] = ''
            
            # ä¿å­˜åˆ°Excel
            df.to_excel(self.analysis_path, index=False)
            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜è‡³: {self.analysis_path}")
            logger.info(f"ä¿å­˜çš„æ•°æ®è¡Œæ•°: {len(df)}")
            logger.info(f"ä¿å­˜çš„æ•°æ®å†…å®¹: {analysis_results}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æç»“æœæ—¶å‡ºé”™: {e}")
            logger.exception(e)

    async def process_history_file(self):
        """å¤„ç†å†å²æ•°æ®æ–‡ä»¶"""
        try:
            logger.info("å¼€å§‹å¤„ç†meme.xlsxå†å²æ•°æ®")
            
            if not self.meme_path.exists():
                logger.error("meme.xlsxæ–‡ä»¶ä¸å­˜åœ¨")
                return
                
            df = pd.read_excel(self.meme_path)
            logger.info(f"åŠ è½½äº† {len(df)} æ¡å†å²è®°å½•")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            if 'å†…å®¹' not in df.columns:
                logger.error("meme.xlsxæ–‡ä»¶ç¼ºå°‘'å†…å®¹'åˆ—")
                return
                
            # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
            for _, row in df.iterrows():
                content = row['å†…å®¹']
                logger.info(f"å¤„ç†å…³é”®è¯: {content}")
                
                # æœç´¢Twitter
                tweets = await twitter_api.search_tweets(content)
                logger.info(f"æ‰¾åˆ° {len(tweets)} æ¡ç›¸å…³æ¨æ–‡")
                
                if tweets:
                    # åˆ†ææ¨æ–‡
                    analysis = await self.analyze_tweets(content, tweets)
                    if analysis:
                        await self.save_analysis_results([analysis])
                        logger.info(f"å·²ä¿å­˜å…³é”®è¯ '{content}' çš„åˆ†æç»“æœ")
                    else:
                        logger.warning(f"å…³é”®è¯ '{content}' çš„åˆ†æç»“æœä¸ºç©º")
                else:
                    logger.warning(f"å…³é”®è¯ '{content}' æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ¨æ–‡")
            
            logger.info("å†å²æ•°æ®å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å¤„ç†å†å²æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            logger.exception(e)

class BacktestProcessor:
    def __init__(self):
        self.data_dir = Path('data')
        self.data_dir.mkdir(exist_ok=True)
        self.meme_path = self.data_dir / 'meme.xlsx'
        self.twitter_results_path = self.data_dir / 'twitter_results.xlsx'
        
    async def save_meme_data(self, meme_data):
        """ä¿å­˜memeæ•°æ®åˆ°Excel"""
        try:
            if self.meme_path.exists():
                df_meme = pd.read_excel(self.meme_path)
                df_meme = pd.concat([df_meme, pd.DataFrame(meme_data)], ignore_index=True)
            else:
                df_meme = pd.DataFrame(meme_data)
            
            df_meme.to_excel(self.meme_path, index=False)
            logger.info(f"æˆåŠŸä¿å­˜ {len(meme_data)} æ¡memeæ•°æ®åˆ°Excel")
        except Exception as e:
            logger.error(f"ä¿å­˜memeæ•°æ®æ—¶å‡ºé”™: {e}")

# åˆ›å»ºå…¨å±€å®ä¾‹
processor = BacktestProcessor()

async def process_message(message_data: Dict[str, Any]) -> None:
    """å¤„ç†æ¥è‡ªDiscordçš„æ¶ˆæ¯"""
    try:
        # æå–å¤„ç†å¥½çš„æ•°æ®
        meme_data = message_data.get('meme_data', [])
        search_terms = message_data.get('search_terms', [])
        
        # ä¿å­˜memeæ•°æ®
        if meme_data:
            await processor.save_meme_data(meme_data)
            
    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯æ•°æ®æ—¶å‡ºé”™: {str(e)}")

async def main():
    try:
        analyzer = MemeAnalyzer()
        await analyzer.process_history_file()
        
        # æ·»åŠ æ–‡ä»¶ç›‘æ§
        monitor = MemeAnalysisMonitor()
        # åˆ›å»ºæ–°çº¿ç¨‹è¿è¡Œç›‘æ§ï¼Œè¿™æ ·ä¸ä¼šé˜»å¡ä¸»ç¨‹åº
        import threading
        monitor_thread = threading.Thread(
            target=monitor.monitor_analysis_file,
            args=(5,),  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
            daemon=True
        )
        monitor_thread.start()
        
    except Exception as e:
        logger.error(f"è¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.exception(e)

# åªåœ¨ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œmainå‡½æ•°
if __name__ == '__main__':
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())

class MemeAnalysisMonitor:
    def __init__(self):
        # åˆå§‹åŒ–é£ä¹¦å®¢æˆ·ç«¯
        self.client = lark.Client.builder() \
            .app_id("cli_a736cea2ff78100d") \
            .app_secret("C9FsC6CnJz3CLf0PEz0NQewkuH6uvCdS") \
            .log_level(lark.LogLevel.DEBUG) \
            .build()
            
        self.alert_chat_id = "oc_a2d2c5616c900bda2ab8e13a77361287"
        self.data_dir = Path('data')
        self.analysis_file = self.data_dir / 'crypto_analysis_results.xlsx'
        self.last_modified_time = None
        self.processed_rows = set()

    def send_message(self, message: str) -> bool:
        """ä½¿ç”¨APIæ–¹å¼å‘é€æ¶ˆæ¯åˆ°é£ä¹¦ç¾¤"""
        try:
            # æ„é€ æ¶ˆæ¯å†…å®¹
            content = json.dumps({"text": message})
            
            # æ„é€ è¯·æ±‚å¯¹è±¡
            request = CreateMessageRequest.builder() \
                .receive_id_type("chat_id") \
                .request_body(CreateMessageRequestBody.builder()
                    .receive_id(self.alert_chat_id)
                    .msg_type("text")
                    .content(content)
                    .build()) \
                .build()

            # å‘é€æ¶ˆæ¯
            response = self.client.im.v1.message.create(request)

            # å¤„ç†å“åº”
            if not response.success():
                logging.error(
                    f"å‘é€æ¶ˆæ¯å¤±è´¥ï¼Œé”™è¯¯ç : {response.code}, "
                    f"é”™è¯¯ä¿¡æ¯: {response.msg}, "
                    f"æ—¥å¿—ID: {response.get_log_id()}"
                )
                return False

            logging.info("æ¶ˆæ¯å‘é€æˆåŠŸ")
            return True

        except Exception as e:
            logging.error(f"å‘é€æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            logging.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return False

    def monitor_analysis_file(self, interval: int = 5):
        """
        ç›‘æ§ crypto_analysis_results.xlsx æ–‡ä»¶çš„æ›´æ–°
        :param interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        logging.info(f"å¼€å§‹ç›‘æ§æ–‡ä»¶: {self.analysis_file}")
        
        while True:
            try:
                if not self.analysis_file.exists():
                    logging.warning("åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨")
                    time.sleep(interval)
                    continue

                current_mtime = os.path.getmtime(self.analysis_file)
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ›´æ–°
                if self.last_modified_time is None or current_mtime > self.last_modified_time:
                    logging.info("æ£€æµ‹åˆ°æ–‡ä»¶æ›´æ–°ï¼Œå¤„ç†æ–°æ•°æ®...")
                    self._process_new_data()
                    self.last_modified_time = current_mtime
                
                time.sleep(interval)
                
            except Exception as e:
                logging.error(f"ç›‘æ§æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                time.sleep(interval)

    def _process_new_data(self):
        """å¤„ç†æ–°çš„åˆ†ææ•°æ®"""
        try:
            df = pd.read_excel(self.analysis_file)
            
            # å¤„ç†æ¯ä¸€è¡Œæ–°æ•°æ®
            for _, row in df.iterrows():
                # åˆ›å»ºè¡Œçš„å”¯ä¸€æ ‡è¯†ï¼ˆä½¿ç”¨æœç´¢å…³é”®è¯å’Œåˆ†ææ—¶é—´çš„ç»„åˆï¼‰
                row_id = f"{row['æœç´¢å…³é”®è¯']}_{row['åˆ†ææ—¶é—´']}"
                
                # å¦‚æœè¿™è¡Œæ•°æ®å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡
                if row_id in self.processed_rows:
                    continue
                
                # æ„é€ é¢„è­¦æ¶ˆæ¯
                alert_msg = (
                    f"ğŸ” æ–°çš„Memeå¸åˆ†æç»“æœ\n\n"
                    f"ğŸ“Œ å…³é”®è¯: {row['æœç´¢å…³é”®è¯']}\n"
                    f"ğŸ“ å™äº‹ä¿¡æ¯: {row['å™äº‹ä¿¡æ¯']}\n\n"
                    f"ğŸŒ¡ï¸ å¯æŒç»­æ€§åˆ†æ:\n"
                    f"â€¢ ç¤¾åŒºçƒ­åº¦: {row['å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦']}\n"
                    f"â€¢ ä¼ æ’­æ½œåŠ›: {row['å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›']}\n"
                    f"â€¢ çŸ­æœŸæŠ•æœºä»·å€¼: {row['å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼']}\n\n"
                    f"ğŸ“Š æ•°æ®ç»Ÿè®¡:\n"
                    f"â€¢ åŸå§‹æ¨æ–‡æ•°é‡: {row['åŸå§‹æ¨æ–‡æ•°é‡']}\n"
                    f"â€¢ åˆ†ææ—¶é—´: {row['åˆ†ææ—¶é—´']}"
                )
                
                # å‘é€é¢„è­¦
                if self.send_message(alert_msg):
                    logging.info(f"å·²å‘é€æ–°åˆ†æç»“æœé¢„è­¦: {row['æœç´¢å…³é”®è¯']}")
                    # æ ‡è®°è¯¥è¡Œæ•°æ®ä¸ºå·²å¤„ç†
                    self.processed_rows.add(row_id)
                else:
                    logging.error(f"å‘é€é¢„è­¦å¤±è´¥: {row['æœç´¢å…³é”®è¯']}")
                
                # å¦‚æœå·²å¤„ç†çš„è¡Œæ•°è¿‡å¤šï¼Œæ¸…ç†æ—§æ•°æ®
                if len(self.processed_rows) > 1000:
                    self.processed_rows = set(list(self.processed_rows)[-500:])
                
        except Exception as e:
            logging.error(f"å¤„ç†æ–°æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")